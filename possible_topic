# Intro
**What was the initial motivation**
There are modern research attempts (see https://arxiv.org/abs/1802.09979 and the works around) to "explain" the rapid success of deep neural networks (mostly euclidean background) via the random matrix theory. On the other hand, noncommutative mathematics offers the means to extend these results to general curved background with the possible extension to quantum spaces (seen as noncommutative operator algebras).

**What do we want?**
develop rational design principles for neural network architectures on general curved background with the extendability to quantum situation, i.e.
1. how to choose architecture to speed up the learning ?
2. how to avoid exploding/vanishing gradients?
3. how to avoid being trapped in the local minima during trainin?
4. how many layers and nodes in each layer?
5. how we can transfer learning to different data (steerability)?
